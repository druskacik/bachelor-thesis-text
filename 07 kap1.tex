\section{Lineárny regresný model}
\label{linear regression}
 
Majme $n$ nameraných štatistických jednotiek tvaru $\{ y, x_1, \ldots, x_p \}$, 
ktoré sme dostali ako výsledok experimentu. 
Lineárny regresný model predpokladá, že medzi jednotlivými prvkami $y, x_1, ... x_p$ je lineárny vzťah. 
Motiváciou za lineárnym regresným modelom je spravidla aproximovať tento lineárny vzťah.

Aproximácia lineárneho vzťahu nám v praxi ponúkne mechanizmus, 
ktorým možno predikovať neznámu hodnotu $y$ na základe známych hodnôt $y, x_1, ... x_p$, 
čo v reálnom živote predstavuje často sa vyskytujúci problém.

Označme teda daný lineárny vzťah medzi zložkami nameranej štatistickej jednotky:

\begin{center}
$
y_i = b_0 + b_1 x_{i_1} + … + b_p x_{i_p} + e_i = b^T x_i + e_i
$
\end{center}

kde $\{ y_i, x_i \}$ je $i$-ta nameraná jednotka, $b$ je vektor lineárneho vzťahu
a $e_i$ je chyba merania.

Keď lineárne vzťahy pre každú z $n$ nameraných jednotiek zapíšeme maticovo, dostaneme vzťah
\begin{align}
\label{linear regression formula}
y = Xb + e
\end{align}

kde $y = (y_1, y_2, \ldots, y_n)^T$, $e = (e_1, e_2, \ldots, e_3)^T$ a

\begin{center}
$
X =
\begin{bmatrix}
x_{11} & \ldots & x_{1p} \\
\vdots & \ddots & \\
x_{n1} & \ldots & x_{np} 
\end{bmatrix}
$
\end{center}

je matica tvaru $n \times p$. V praxi je $b$ neznámy vektor, ktorý sa snažíme odhadnúť.

Na spočítanie odhadu $b$ sa používajú rôzne metódy,
najčastejšie napr. metóda najmenších štvorcov alebo metóda maximálnej vierohodnosti.

\subsection{Metóda najmenších štvorcov}

Metódou najmenších štvorcov vypočítame odhad $\hat{b}$ parametra $b$ nasledovne:

\begin{center}
$
\hat{b} = \underset{b \in \mathbb{R}^{p}}{\operatorname{arg min}} (y - Xb)^T C (y - Xb) =
\underset{b \in \mathbb{R}^{p}}{\operatorname{arg min}} ||y - Xb||_{C^{-1}}^2
$
\end{center}

kde $C$ je nejaká kladne definitná matica. 
Ak $C = I$, potom minimalizujeme výraz
$||y - Xb||_I^2 = ||y - Xb||^2 = \sum_{i=1}^n (y_i - X_{i \cdot } b)^2$
, kde $X_{i \cdot }$ značí $i$-ty riadok matice $X$.
V našej práci budeme predpokladať homogenitu chýb, čo v praxi znamená,
že skutočne budeme môcť dosadiť $C = I$.
Preto maticu $C$ v ďalšom opise teórie spomínať nebudeme.

Geometricky metódu najmenších štvorcov možno interpretovať ako projekciu vektora $y$ 
na stĺpcový priestor matice $X$. Hľadáme teda taký vektor $\hat{b}$, 
pre ktorý platí $X \hat{b} = Py$, kde $P$ je matica ortogonálnej projekcie na stĺpcový priestor $X$. 
Z teórie lineárnej algebry vieme, že $P = X (X^T X)^- X^T$, kde znamienko $^-$ označuje $g$-inverziu.
($g$-inverziou matice $A$ je taká matica $A^-$, pre ktorú platí $A A^- A = A$).

Odhad $\hat{b}$ parametra $b$ je teda riešením rovnice
\begin{align}
\label{least squares solution}
X b =  X (X^T X)^- X^T y
\end{align}

Toto riešenie spočítame ako:

\begin{center}
$
\hat{b} = (X^T X)^- X^T y
$
\end{center}
kde použitá $g$-inverzia je ľubovoľná.

Z uvedeného vyplýva, že v prípade regulárnosti $X$ je odhad $\hat{b}$ jednoznačný. 
V našej práci budeme skúmať matice (modely) $X$, ktoré nie sú regulárne, 
takže jednoznačný odhad $\hat{b}$ nebudeme schopní nájsť (čo v konečnom dôsledku ani nie je naším záujmom).
Budeme odhadovať lineárnu funkciu zložiek vektora $b$, konkrétne $h^T b = h_1 b_1 + \ldots + h_p b_p$,
ktorá býva odhadnuteľná aj v prípade singularity $X$, ak vektor $h$ spĺňa určité podmienky. 
Je niekoľko ekvivalentných podmienok, ktoré stačia na to, aby $h^T b$ bolo odhadnuteľné. 
Z nich spomenieme jednu v nasledovnej vete, ktorú použijeme neskôr v našej práci.

\begin{theorem}
\label{veta1}
$h^T b$ je odhadnuteľné, ak plati nasledovné ekvivaltentné podmienky:
\begin{enumerate}
  \item pre ľubovoľné riešenia $b^*$ a $b^{**}$ rovnice (\ref{least squares solution}) platí $h^T b^* = h^T b^{**}$
  \item $h \in \mathcal{M}(X^T)$
  \item $h \in \mathcal{M}(X^T X)$,
\end{enumerate}
kde $\mathcal{M}$ označuje stĺpcový priestor matice.
\end{theorem}

Ak $h$ patrí do riadkového priestoru matice $X$, potom existuje také $u$, že $h = F^T u$.
Potom pre jednoznačný odhad $h^T \hat{b}$ vektora $h^T b$ platí:
% hTÔ = uTFÔ = uTPy = uT F(FTCF)^-FTCy

\begin{center}
$h^T \hat{b} = u^T X \hat{b} = u^T P y = u^T X(X^T X)^- X^T y$
\end{center}

Výsledok predchádzajúcej vety je dôležitý pre našu prácu, pretože nebudeme skúmať odhady $b$,
ale odhady niektorých lineárnych kombinácií zložiek vektora $b$, 
konkrétne napr. rozdiely medzi parametrami.

Odhad $\hat{b}$ parametra $b$, ako aj odhad $h^T \hat{b}$ parametra $h^T b$, sú lineárne nevychýlené odhady,
ktorým prislúcha disperzia (TODO: popremýšľaj, či treba bližšie opísať lineárny nevychýlený odhad). 
Neskôr v našej práci budeme hľadať také modely $X$, pri ktorých je disperzia odhadov $b$ či $h^T b$ najmenšia možná,
čo nám dá najlepší lineárny nevychýlený odhad. 
Ak nami navrhované modely $X$ budú opisovať ten istý experiment, ten model $X$,
pre ktorý disperzia odhadu $h^T b$ bude najmenšia, bude svojím spôsobom optimálny.

K nájdeniu optimálneho modelu $X$ nám poslúži Gaussova-Markovova veta, 
ktorá určuje minimálnu možnú disperziu odhadu $h^T b$.

\begin{theorem}
\label{gauss-markov}
(Gaussova-Markovova) Nech $h$ je z riadkového priestoru $X$. 
Potom minimálna možná disperzia lineárneho nevychýleného odhadu $h^T b$ je

\begin{center}
$
m = \mathrm{Var}[h^T b] = h^T M^- h
$
\end{center}

kde $M = X^T X$ je informačná matica parametra $b$ a $M^-$ je jej ľubovoľná $g$-inverzia.
\end{theorem}

\subsection{Metodika skúmania}

Cieľom nášho skúmania bude nájsť ideálnu maticu vstupu do experimentu (konkrétny problém popíšeme v nasledujúcej kapitole). 
Pokúsime sa vysloviť závery pre všeobecný prípad matice $m \times n$,
avšak pre istý prvotný náhľad do problematiky problém popíšeme a preskúmame na menších rozmeroch, 
konkrétne do rozmeru, do ktorého nám to umožní výpočtová technika. Pracovať budeme s programovacím jazykom \textit{python}.

Od „preskúmania situácie“ do istého rozmeru si sľubujeme dôležité náhľady, 
ktoré nám umožnia vysloviť závery a hypotézy pre všeobecný prípad $m \times n$. 
Tie sa následne pokúsime dokázať matematickými metódami.
