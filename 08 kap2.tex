\section{Lineárny regresný model}
\label{linear regression}
 
Majme $n$ nameraných štatistických jednotiek tvaru $\{ y, x_1, \ldots, x_p \}$, 
ktoré sme dostali ako výsledok experimentu. 
Lineárny regresný model predpokladá, že medzi jednotlivými prvkami $y, x_1, ... x_p$ je lineárny vzťah. 
Motiváciou za lineárnym regresným modelom je spravidla aproximovať tento lineárny vzťah.

Aproximácia lineárneho vzťahu nám v praxi ponúkne mechanizmus, 
ktorým možno predikovať neznámu hodnotu $y$ na základe známych hodnôt $y, x_1, ... x_p$, 
čo v reálnom živote predstavuje často sa vyskytujúci problém.

Označme teda daný lineárny vzťah medzi zložkami nameranej štatistickej jednotky:

\begin{center}
$
y_i = b_0 + b_1 x_{i_1} + … + b_p x_{i_p} + e_i = b^T x_i + e_i
$
\end{center}

kde $\{ y_i, x_i \}$ je $i$-ta nameraná jednotka, $b$ je vektor lineárneho vzťahu
a $e_i$ je chyba merania.

Keď lineárne vzťahy pre každú z $n$ nameraných jednotiek zapíšeme maticovo, dostaneme vzťah
\begin{align}
\label{linear regression formula}
y = Xb + e
\end{align}

kde $y = (y_1, y_2, \ldots, y_n)^T$, $e = (e_1, e_2, \ldots, e_n)^T$ a

\begin{center}
$
X =
\begin{bmatrix}
1 & x_{11} & \ldots & x_{1p} \\
\vdots & \vdots & \ddots & \\
1 & x_{n1} & \ldots & x_{np} 
\end{bmatrix}
$
\end{center}

je matica tvaru $n \times p$. V praxi je $b$ neznámy vektor, ktorý sa snažíme odhadnúť.

Na spočítanie odhadu $b$ sa používajú rôzne metódy,
najčastejšie napr. metóda najmenších štvorcov alebo metóda maximálnej vierohodnosti.

\subsection{Metóda najmenších štvorcov}

Metódou najmenších štvorcov vypočítame odhad $\hat{b}$ parametra $b$ nasledovne:

\begin{center}
$
\hat{b} = \underset{b \in \mathbb{R}^{p}}{\operatorname{arg min}} (y - Xb)^T C (y - Xb) =
\underset{b \in \mathbb{R}^{p}}{\operatorname{arg min}} ||y - Xb||_{C^{-1}}^2
$
\end{center}

kde $C$ je nejaká kladne definitná matica. 
Ak $C = I$, potom minimalizujeme výraz
$||y - Xb||_I^2 = ||y - Xb||^2 = \sum_{i=1}^n (y_i - X_{i \cdot } b)^2$
, kde $X_{i \cdot }$ značí $i$-ty riadok matice $X$.
V našej práci budeme predpokladať nezávislosť chýb a tiež homogenitu ich rozptylu, čo v praxi znamená,
že skutočne budeme môcť dosadiť $C = I$.
Preto maticu $C$ v ďalšom opise teórie spomínať nebudeme.

Geometricky metódu najmenších štvorcov možno interpretovať ako projekciu vektora $y$ 
na stĺpcový priestor matice $X$. Hľadáme teda taký vektor $\hat{b}$, 
pre ktorý platí $X \hat{b} = Py$, kde $P$ je matica ortogonálnej projekcie na stĺpcový priestor $X$. 
Z teórie lineárnej algebry vieme, že $P = X (X^T X)^- X^T$, kde znamienko $^-$ označuje $g$-inverziu.
($g$-inverziou matice $A$ je taká matica $A^-$, pre ktorú platí $A A^- A = A$).

Odhad $\hat{b}$ parametra $b$ je teda riešením rovnice
\begin{align}
\label{least squares solution}
X b =  X (X^T X)^- X^T y
\end{align}

kde $P = X (X^T X)^- X^T$ je ortogonálny projektor na stĺpcový priestor $X$.

Všetky riešenia tejto rovnice musia mať tvar:

\begin{center}
$
(X^T X)^- X^T y
$
\end{center}

kde použitá $g$-inverzia je ľubovoľná.

Z uvedeného vyplýva, že v prípade regulárnosti $X^T X$ je odhad $\hat{b}$ parametra $b$ jednoznačný. 
V našej práci budeme skúmať matice (modely) $X$, ktoré nie sú regulárne, 
takže jednoznačný odhad $\hat{b}$ nebudeme schopní nájsť (čo v konečnom dôsledku ani nie je naším záujmom).
Budeme odhadovať lineárnu funkciu zložiek vektora $b$, konkrétne $h^T b = h_1 b_1 + \ldots + h_p b_p$,
ktorá býva jednoznačne odhadnuteľná aj v prípade singularity $X^T X$, ak vektor $h$ spĺňa určité podmienky. 

\begin{defin}
\label{odhadnutelnost}
Lineárnu kombináciu $h^T b$ zložiek vektora b nazývame odhadnuteľnou,
ak pre ľubovoľné riešenia $b^*$ a $b^{**}$ rovnice (\ref{least squares solution}) platí $h^T b^* = h^T b^{**}$.
\end{defin}

Je niekoľko ekvivalentných podmienok, ktoré stačia na to, aby $h^T b$ bolo odhadnuteľné. 
Z nich spomenieme dve v nasledovnej vete, ktorú použijeme neskôr v našej práci.

\begin{theorem}
\label{veta1}
$h^T b$ je odhadnuteľné, ak platia nasledovné ekvivalentné podmienky:
\begin{enumerate}
  \item $h \in \mathcal{M}(X^T)$
  \item $h \in \mathcal{M}(X^T X)$,
\end{enumerate}
kde $\mathcal{M}$ označuje stĺpcový priestor matice.
\end{theorem}

\begin{dokaz}
Ukážeme, že ak $h \in \mathcal{M}(X^T)$ tak $h^T b$ je odhadnuteľné.
Nech teda $h \in \mathcal{M}(X^T)$. Potom existuje také $u$, že $h = X^T u$.
Preto pre ľubovoľné riešenie $\hat{b}$ rovnice (\ref{least squares solution}) platí:

\begin{center}
$
h^T \hat{b} = u^T X \hat{b} = u^T P y
$
\end{center}

kde päta kolmice $Py$ je jednoznačne daná, preto aj $h^T \hat{b}$ je jednoznačne dané (bez ohľadu na voľbu $\hat{b}$).
$h^T b$ je teda podľa definície (\ref{odhadnutelnost}) odhadnuteľné.

Ekvivalencia podmienok (1) a (2) vyplýva z toho, že podľa lemy (\ref{o stlpcovych priestoroch}) vo všeobecnosti platí, že $\mathcal{M}(X^T) = \mathcal{M}(X^T X)$.
$\square$
\end{dokaz}

Ak $h$ patrí do riadkového priestoru matice $X$, potom existuje také $u$, že $h = X^T u$.
Potom pre jednoznačný odhad $h^T \hat{b}$ vektora $h^T b$ platí:

\begin{center}
$h^T \hat{b} = u^T X \hat{b} = u^T P y = u^T X(X^T X)^- X^T y$
\end{center}

Vidíme, že v strede výrazu na pravej strane je projekčná matica $X(X^T X)^- X^T$, ktorá je vždy jednoznačne určená, 
nezávisle na voľbe zovšeobecnenej $g$-inverzie v danom výraze.

Výsledok predchádzajúcej vety je dôležitý pre našu prácu, pretože nebudeme skúmať odhady $b$,
ale odhady niektorých lineárnych kombinácií zložiek vektora $b$, 
konkrétne napr. rozdiely medzi parametrami.

Odhad $\hat{b}$ parametra $b$, ako aj odhad $h^T \hat{b}$ parametra $h^T b$, sú lineárne nevychýlené odhady,
ktorým prislúcha disperzia (TODO: popremýšľaj, či treba bližšie opísať lineárny nevychýlený odhad). 
Neskôr v našej práci budeme hľadať také modely $X$, pri ktorých je disperzia odhadov $h^T b$ najmenšia možná,
čo nám dá najlepší lineárny nevychýlený odhad. 
Ak nami navrhované modely $X$ budú opisovať ten istý experiment, ten model $X$,
pre ktorý disperzia odhadu $h^T b$ bude najmenšia, bude svojím spôsobom optimálny.

K nájdeniu optimálneho modelu $X$ nám poslúži Gaussova-Markovova veta, 
ktorá určuje minimálnu možnú disperziu odhadu $h^T b$.

\begin{theorem}
\label{gauss-markov}
(Gaussova-Markovova) Nech $h$ je z riadkového priestoru $X$. 
Potom minimálna možná disperzia lineárneho nevychýleného odhadu $h^T b$ je

\begin{center}
$
m = h^T M^- h
$
\end{center}

kde $M = X^T X$ je informačná matica parametra $b$ a $M^-$ je jej ľubovoľná $g$-inverzia.
\end{theorem}