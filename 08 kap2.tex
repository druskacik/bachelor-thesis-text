\section{Lineárny regresný model}
\label{linear regression}
 
Lineárna regresia patrí v súčasnosti medzi najpoužívanejšie štatistické metódy. Aplikácie regresie možno nájsť v mnohých vedeckých odvetviach, 
vrátane medicíny, biológie, ekonómie, sociológie atď. Ciele regresnej analýzy zväčša možno zaradiť do troch oblastí:

\begin{enumerate}
  \item nájsť vzťah medzi závislou premennou $y$ a regresormi $x_1, \ldots, x_n$,
  \item odhadnúť $y$ na základe hodnôt $x_1, \ldots, x_n$,
  \item preskúmať premenné $x_1, \ldots, x_n$ a identifikovať, ktoré z nich lepšie popisujú premennú $y$, resp. určiť, ktoré z nich majú na $y$ aký vplyv
\end{enumerate}

V našej práci sa budeme zaoberať treťou z pomenovaných oblastí.

Uvažujme teda lineáry regresný model

\begin{align}
\label{linear regression formula}
y = Xb + \varepsilon
\end{align}

Na ľavej strane $y$ je $n \times 1$ vektor pozorovaných náhodných premenných. Vektor $y$ nazývame \emph{závislou} premennou.
Na pravej strane $X$ je matica plánu, $b$ je neznámy parameter regresného modelu a $\varepsilon$ je náhodná chyba.
$X$ vo všeobecnosti môže, ale nemusí byť náhodná matica. Pre účely našej práce budeme narábať s fixnou (známou) maticou $X$,
čo zohľadníme aj v ďalších teoretických základoch v tejto kapitole.

Nech teda $X$ je známa matica $n \times p$ tvaru:

\begin{center}
$
X =
\begin{bmatrix}
1 & x_{11} & \ldots & x_{1p} \\
\vdots & \vdots & \ddots & \\
1 & x_{n1} & \ldots & x_{np} 
\end{bmatrix}
$
\end{center}

Rovnica (\ref{linear regression formula}) nám určuje $n$ lineárnych vzťahov tvaru:

\begin{center}
$
y_i = b_0 + b_1 x_{i1} + … + b_p x_{ip} + {\varepsilon}_i = b^T x_i + {\varepsilon}_i
$
\end{center}

pre $y = (y_1, y_2, \ldots, y_n)^T$, $b = (b_1, b_2, \ldots, b_p)^T$ a $\varepsilon = ({\varepsilon}_1, {\varepsilon}_2, \ldots, {\varepsilon}_n)^T$.

Ďalej budeme predpokladať $E(\varepsilon) = \textbf{0}$, $Var[\varepsilon] = {\sigma}^2 I$,
t. j jednotlivé ${\varepsilon}_i, {\varepsilon}_j$ sú nezávislé a rovnako rozdelené.
Pre jednoduchosť takisto predpokladajme, že ${\sigma}^2 = 1$.

V praxi je $b$ neznámy vektor, ktorý sa snažíme odhadnúť na základe nameraných hodnôt $y$.
Na spočítanie odhadu sa používajú rôzne metódy, najčastejšie napr. metóda najmenších štvorcov alebo metóda maximálnej vierohodnosti.

\subsection{Metóda najmenších štvorcov}

Metódou najmenších štvorcov vypočítame odhad $\hat{b}$ parametra $b$ nasledovne:

\begin{center}
$
\hat{b} = \underset{b \in \mathbb{R}^{p}}{\operatorname{arg min}} (y - Xb)^T C (y - Xb) =
\underset{b \in \mathbb{R}^{p}}{\operatorname{arg min}} ||y - Xb||_{C^{-1}}^2
$
\end{center}

kde $C$ je nejaká kladne definitná matica. 
Ak $C = I$, potom minimalizujeme výraz
$||y - Xb||_I^2 = ||y - Xb||^2 = \sum_{i=1}^n (y_i - X_{i \cdot } b)^2$
, kde $X_{i \cdot }$ značí $i$-ty riadok matice $X$.
Keďže sme predpokladali nezávislosť chýb a tiež homogenitu ich rozptylu, je pre nás najvýhodnejšie dosadiť $C = I$,
čo možno dokázať pomocou Gaussovej-Markovovej vety, ktorú uvedieme neskôr.
Preto maticu $C$ v ďalšom opise teórie spomínať nebudeme.

Geometricky metódu najmenších štvorcov možno interpretovať ako projekciu vektora $y$ 
na stĺpcový priestor matice $X$. Hľadáme teda taký vektor $\hat{b}$, 
pre ktorý platí $X \hat{b} = Py$, kde $P$ je matica ortogonálnej projekcie na stĺpcový priestor $X$. 
Z vety (\ref{ortogonalny projektor na maticu}) vieme, že $P = X (X^T X)^- X^T$.

Odhad $\hat{b}$ parametra $b$ je teda riešením rovnice
\begin{align}
\label{least squares solution}
X b =  X (X^T X)^- X^T y
\end{align}

kde $P = X (X^T X)^- X^T$ je ortogonálny projektor na stĺpcový priestor $X$.

Všetky riešenia tejto rovnice musia mať tvar:

\begin{center}
$
(X^T X)^- X^T y
$
\end{center}

kde použitá $g$-inverzia je ľubovoľná.

Z uvedeného vyplýva, že v prípade regulárnosti $X^T X$ je odhad $\hat{b}$ parametra $b$ jednoznačný. 
V našej práci budeme skúmať matice $X$, ktoré nie sú regulárne, 
takže jednoznačný odhad $\hat{b}$ nebudeme schopní nájsť (čo v konečnom dôsledku ani nie je naším záujmom).
Budeme odhadovať lineárnu funkciu zložiek vektora $b$, konkrétne $h^T b = h_1 b_1 + \ldots + h_p b_p$,
ktorá býva jednoznačne odhadnuteľná aj v prípade singularity $X^T X$, ak vektor $h$ spĺňa určité podmienky. 

\begin{defin}
\label{odhadnutelnost}
Lineárnu kombináciu $h^T b$ zložiek vektora b nazývame odhadnuteľnou,
ak pre ľubovoľné riešenia $b^*$ a $b^{**}$ rovnice (\ref{least squares solution}) platí $h^T b^* = h^T b^{**}$.
\end{defin}

Je niekoľko ekvivalentných podmienok, ktoré stačia na to, aby $h^T b$ bolo odhadnuteľné. 
Z nich spomenieme dve v nasledovnej vete, ktorú použijeme neskôr v našej práci.

\begin{theorem}
\label{veta1}
$h^T b$ je odhadnuteľné, ak platia nasledovné ekvivalentné podmienky:
\begin{enumerate}
  \item $h \in \mathcal{M}(X^T)$
  \item $h \in \mathcal{M}(X^T X)$,
\end{enumerate}
kde $\mathcal{M}$ označuje stĺpcový priestor matice.
\end{theorem}

\begin{dokaz}
Ukážeme, že ak $h \in \mathcal{M}(X^T)$ tak $h^T b$ je odhadnuteľné.
Nech teda $h \in \mathcal{M}(X^T)$. Potom existuje také $u$, že $h = X^T u$.
Preto pre ľubovoľné riešenie $\hat{b}$ rovnice (\ref{least squares solution}) platí:

\begin{center}
$
h^T \hat{b} = u^T X \hat{b} = u^T P y
$
\end{center}

kde päta kolmice $Py$ je jednoznačne daná, preto aj $h^T \hat{b}$ je jednoznačne dané (bez ohľadu na voľbu $\hat{b}$).
$h^T b$ je teda podľa definície (\ref{odhadnutelnost}) odhadnuteľné.

Ekvivalencia podmienok (1) a (2) vyplýva z toho, že podľa lemy (\ref{o stlpcovych priestoroch}) vo všeobecnosti platí, že $\mathcal{M}(X^T) = \mathcal{M}(X^T X)$.
$\square$
\end{dokaz}

Ak $h$ patrí do riadkového priestoru matice $X$, potom existuje také $u$, že $h = X^T u$.
Potom pre jednoznačný odhad $h^T \hat{b}$ vektora $h^T b$ platí:

\begin{center}
$h^T \hat{b} = u^T X \hat{b} = u^T P y = u^T X(X^T X)^- X^T y$
\end{center}

Vidíme, že v strede výrazu na pravej strane je projekčná matica $X(X^T X)^- X^T$, ktorá je vždy jednoznačne určená, 
nezávisle na voľbe zovšeobecnenej $g$-inverzie v danom výraze.

Výsledok predchádzajúcej vety je dôležitý pre našu prácu, pretože nebudeme skúmať odhady $b$,
ale odhady niektorých lineárnych kombinácií zložiek vektora $b$, 
konkrétne napr. rozdiely medzi nimi.

\subsection{Gaussova-Markovova veta}

V predchádzajúcej časti sme ukázali, že ak matica je matica plánu $X$ regulárna,
jej odhad metódou najmenších štvorcov je rovný

\begin{center}
$
\hat{b} = \underset{b \in \mathbb{R}^{p}}{\operatorname{arg min}} ||y - Xb||^2 = (X^T X)^{-1} X^T y
$
\end{center}

Pre strednú hodnotu odhadu $\hat{b}$ teda platí:

\begin{center}
$
E[\hat{b}] = E[(X^T X)^{-1} X^T y] = (X^T X)^{-1} X^T E[y] = (X^T X)^- X^T E[Xb + \varepsilon] = (X^T X)^{-1} X^T X b = b
$
\end{center}

To znamená, že $\hat{b}$ je lineárnym nevychýleným odhadom parametra $b$.
Pre varianciu $\hat{b}$ dostávame:

\begin{center}
$
Var[\hat{b}] = Var[(X^T X)^{-1} X^T y] = (X^T X)^{-1} X^T Var[Xb + \varepsilon] X (X^T X)^{-1} =
(X^T X)^{-1} X^T Var[\varepsilon] X (X^T X)^{-1} = (X^T X)^{-1} X^T I X (X^T X)^{-1} = (X^T X)^{-1}
$.
\end{center}

Veľkosť kovariančnej matice $(X^T X)^{-1}$ je v určitom zmysle najmenšia možná, t. j. $\hat{b}$ je najlepší lineárne nevychýlený odhad.
Podobné tvrdenie platí pre odhad $h^T \hat{b}$ parametra $h^T b$, ktorý je takisto lineárne nevychýlený. Upresníme to v nasledujúcej vete.

Neskôr v našej práci budeme hľadať také modely $X$, pri ktorých je disperzia odhadov $h^T b$ najmenšia možná,
čo nám dá najlepší lineárny nevychýlený odhad. 
Ak nami navrhované modely $X$ budú opisovať ten istý experiment, ten model $X$,
pre ktorý disperzia odhadu $h^T b$ bude najmenšia, bude svojím spôsobom optimálny.

K nájdeniu optimálneho modelu $X$ nám poslúži Gaussova-Markovova veta, 
ktorá určuje minimálnu možnú disperziu odhadu $h^T b$.

\begin{theorem}
\label{gauss-markov}
(Gaussova-Markovova) Nech $h$ je z riadkového priestoru $X$. 
Potom minimálna možná disperzia lineárneho nevychýleného odhadu $h^T b$ je

\begin{center}
$
m = Var[h^T \hat{b}] = h^T M^- h
$
\end{center}

kde $M = X^T X$ je informačná matica parametra $b$ a $M^-$ je jej ľubovoľná $g$-inverzia
a $\hat{b}$ je ľubovoľné riešenie rovnice

\begin{center}
$
(X^T X)b = X^T y
$
\end{center}
\end{theorem}